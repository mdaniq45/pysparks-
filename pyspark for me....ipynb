{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d31a3344-ee1e-45ba-9793-a55752e3b32c",
   "metadata": {},
   "source": [
    "\n",
    "### **What is PySpark?**\n",
    "\n",
    "PySpark is the Python API for **Apache Spark**, an open-source distributed computing framework. It enables the processing and analysis of large-scale data across multiple machines using the resilience and scalability of Spark. PySpark is used in big data, machine learning, and real-time data processing.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. PySpark Syllabus**\n",
    "\n",
    "A **typical syllabus for learning PySpark** includes:\n",
    "\n",
    "#### **Basics of PySpark**\n",
    "- Introduction to Apache Spark and PySpark\n",
    "- Installation and Setup of PySpark\n",
    "- Understanding RDD (Resilient Distributed Datasets)\n",
    "- Transformations and Actions in PySpark\n",
    "- Spark Context and Spark Session\n",
    "\n",
    "#### **PySpark SQL**\n",
    "- DataFrames in PySpark\n",
    "- Working with SQL in PySpark\n",
    "- Schema definition and manipulation\n",
    "- Query execution and optimization\n",
    "\n",
    "#### **PySpark Streaming**\n",
    "- Introduction to Spark Streaming\n",
    "- Real-time data processing with DStreams\n",
    "- Integrating with Kafka and Flume\n",
    "\n",
    "#### **Machine Learning with PySpark**\n",
    "- Introduction to MLlib\n",
    "- Supervised and unsupervised learning\n",
    "- Model training, evaluation, and hyperparameter tuning\n",
    "\n",
    "#### **Advanced Topics**\n",
    "- Handling Big Data with PySpark\n",
    "- Partitioning and Shuffling\n",
    "- Performance tuning and optimization\n",
    "- Working with external data sources (S3, HDFS, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "### **2. PySpark Code Examples**\n",
    "\n",
    "#### **Basic PySpark Setup and RDD Creation**\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize Spark Context\n",
    "sc = SparkContext(\"local\", \"PySpark Example\")\n",
    "\n",
    "# Create an RDD\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Apply a transformation and an action\n",
    "squared_rdd = rdd.map(lambda x: x**2)\n",
    "result = squared_rdd.collect()\n",
    "\n",
    "print(\"Squared Numbers:\", result)\n",
    "\n",
    "sc.stop()\n",
    "```\n",
    "\n",
    "#### **Using PySpark SQL for DataFrames**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"PySpark SQL Example\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "data = [(\"John\", 30), (\"Doe\", 25), (\"Jane\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Run SQL Queries\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "result = spark.sql(\"SELECT * FROM people WHERE Age > 30\")\n",
    "\n",
    "result.show()\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "#### **Machine Learning Example**\n",
    "\n",
    "```python\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ML Example\").getOrCreate()\n",
    "\n",
    "# Load Data\n",
    "data = spark.read.format(\"libsvm\").load(\"sample_libsvm_data.txt\")\n",
    "\n",
    "# Train a logistic regression model\n",
    "lr = LogisticRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "lrModel = lr.fit(data)\n",
    "\n",
    "# Print the coefficients and intercept\n",
    "print(\"Coefficients:\", lrModel.coefficients)\n",
    "print(\"Intercept:\", lrModel.intercept)\n",
    "\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Uses of PySpark**\n",
    "\n",
    "#### **Big Data Processing**\n",
    "- PySpark excels in processing vast amounts of data in parallel using its distributed framework.\n",
    "- Examples: ETL pipelines, batch processing, and log analysis.\n",
    "\n",
    "#### **Real-Time Data Processing**\n",
    "- PySpark Streaming is used for real-time data analytics.\n",
    "- Examples: Processing sensor data, clickstream data, or financial transactions.\n",
    "\n",
    "#### **Machine Learning and Analytics**\n",
    "- MLlib (Spark's ML library) supports machine learning tasks like classification, regression, clustering, and recommendation systems.\n",
    "\n",
    "#### **Data Integration and Querying**\n",
    "- PySpark SQL allows querying large datasets using SQL-like syntax, making it ideal for combining structured and unstructured data.\n",
    "\n",
    "#### **Scalable Data Solutions**\n",
    "- PySpark is a backbone of scalable data engineering solutions, handling tasks like distributed computing, fault tolerance, and resource optimization.\n",
    "\n",
    "Would you like to delve deeper into any of these topics or practical applications?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0037c19-5d85-443f-800f-9ee8aab22cf8",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (zama executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Apply a transformation and an action\u001b[39;00m\n\u001b[0;32m     11\u001b[0m squared_rdd \u001b[38;5;241m=\u001b[39m rdd\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m result \u001b[38;5;241m=\u001b[39m squared_rdd\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSquared Numbers:\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n\u001b[0;32m     16\u001b[0m sc\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\rdd.py:1833\u001b[0m, in \u001b[0;36mRDD.collect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[0;32m   1832\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonRDD\u001b[38;5;241m.\u001b[39mcollectAndServe(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd\u001b[38;5;241m.\u001b[39mrdd())\n\u001b[0;32m   1834\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0) (zama executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 17 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "# Initialize Spark Context\n",
    "sc = SparkContext(\"local\", \"PySpark Example\")\n",
    "\n",
    "# Create an RDD\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Apply a transformation and an action\n",
    "squared_rdd = rdd.map(lambda x: x**2)\n",
    "result = squared_rdd.collect()\n",
    "\n",
    "print(\"Squared Numbers:\", result)\n",
    "\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71f48623-2a07-4e12-aa64-5a01cdbff2d4",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o60.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (zama executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m df\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeople\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m result \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT * FROM people WHERE Age > 30\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m result\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     17\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:947\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    888\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[0;32m    889\u001b[0m \n\u001b[0;32m    890\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[0;32m    946\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_show_string(n, truncate, vertical))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\dataframe.py:965\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[1;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m    960\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    961\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvertical\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(vertical)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m    962\u001b[0m     )\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(truncate, \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m truncate:\n\u001b[1;32m--> 965\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mshowString(n, \u001b[38;5;241m20\u001b[39m, vertical)\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    967\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o60.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1.0 (TID 1) (zama executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\r\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\r\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4333)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4323)\r\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\r\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\r\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4321)\r\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3316)\r\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3539)\r\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\r\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:203)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:109)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:124)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:174)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:67)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\r\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\r\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(DualStackPlainSocketImpl.java:135)\r\n\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\r\n\tat java.net.PlainSocketImpl.accept(PlainSocketImpl.java:199)\r\n\tat java.net.ServerSocket.implAccept(ServerSocket.java:545)\r\n\tat java.net.ServerSocket.accept(ServerSocket.java:513)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:190)\r\n\t... 32 more\r\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"PySpark SQL Example\").getOrCreate()\n",
    "\n",
    "# Create a DataFrame\n",
    "data = [(\"John\", 30), (\"Doe\", 25), (\"Jane\", 35)]\n",
    "columns = [\"Name\", \"Age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Run SQL Queries\n",
    "df.createOrReplaceTempView(\"people\")\n",
    "result = spark.sql(\"SELECT * FROM people WHERE Age > 30\")\n",
    "\n",
    "result.show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a04bd28-ac5d-4776-8030-64f319615775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.2\n"
     ]
    }
   ],
   "source": [
    "!python --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85d5698d-7ff1-4224-bbbf-061d81e5708a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.5.3\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: C:\\Users\\hi\\anaconda3\\Lib\\site-packages\n",
      "Requires: py4j\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c53f0f64-9d00-4465-a088-11eb90fb699a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: could not open `C:\\Program Files\\Java\\jre1.8.0_202\\lib\\amd64\\jvm.cfg'\n"
     ]
    }
   ],
   "source": [
    "!java -version\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fbc8e1e-67ec-4429-848c-51348c50e407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\hi\\anaconda3\\lib\\site-packages (3.5.3)\n",
      "Collecting pyspark\n",
      "  Downloading pyspark-3.5.4.tar.gz (317.3 MB)\n",
      "     ---------------------------------------- 0.0/317.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/317.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.8/317.3 MB 3.4 MB/s eta 0:01:35\n",
      "     ---------------------------------------- 1.6/317.3 MB 3.5 MB/s eta 0:01:31\n",
      "     ---------------------------------------- 2.4/317.3 MB 3.6 MB/s eta 0:01:27\n",
      "     ---------------------------------------- 3.4/317.3 MB 4.0 MB/s eta 0:01:18\n",
      "      --------------------------------------- 4.2/317.3 MB 4.0 MB/s eta 0:01:19\n",
      "      --------------------------------------- 5.5/317.3 MB 4.3 MB/s eta 0:01:13\n",
      "      --------------------------------------- 6.8/317.3 MB 4.6 MB/s eta 0:01:09\n",
      "      --------------------------------------- 7.6/317.3 MB 4.5 MB/s eta 0:01:09\n",
      "     - -------------------------------------- 8.7/317.3 MB 4.6 MB/s eta 0:01:07\n",
      "     - -------------------------------------- 9.7/317.3 MB 4.7 MB/s eta 0:01:06\n",
      "     - ------------------------------------- 10.5/317.3 MB 4.7 MB/s eta 0:01:06\n",
      "     - ------------------------------------- 11.5/317.3 MB 4.6 MB/s eta 0:01:07\n",
      "     - ------------------------------------- 12.8/317.3 MB 4.7 MB/s eta 0:01:05\n",
      "     - ------------------------------------- 14.2/317.3 MB 4.8 MB/s eta 0:01:03\n",
      "     - ------------------------------------- 15.5/317.3 MB 4.9 MB/s eta 0:01:02\n",
      "     -- ------------------------------------ 16.5/317.3 MB 5.0 MB/s eta 0:01:01\n",
      "     -- ------------------------------------ 17.6/317.3 MB 4.9 MB/s eta 0:01:02\n",
      "     -- ------------------------------------ 18.6/317.3 MB 4.9 MB/s eta 0:01:01\n",
      "     -- ------------------------------------ 19.7/317.3 MB 4.9 MB/s eta 0:01:01\n",
      "     -- ------------------------------------ 21.0/317.3 MB 5.0 MB/s eta 0:01:00\n",
      "     -- ------------------------------------ 22.3/317.3 MB 5.1 MB/s eta 0:00:59\n",
      "     -- ------------------------------------ 23.6/317.3 MB 5.1 MB/s eta 0:00:58\n",
      "     --- ----------------------------------- 24.9/317.3 MB 5.1 MB/s eta 0:00:57\n",
      "     --- ----------------------------------- 26.2/317.3 MB 5.2 MB/s eta 0:00:56\n",
      "     --- ----------------------------------- 27.3/317.3 MB 5.2 MB/s eta 0:00:56\n",
      "     --- ----------------------------------- 28.6/317.3 MB 5.2 MB/s eta 0:00:56\n",
      "     --- ----------------------------------- 29.6/317.3 MB 5.2 MB/s eta 0:00:55\n",
      "     --- ----------------------------------- 30.9/317.3 MB 5.3 MB/s eta 0:00:55\n",
      "     --- ----------------------------------- 32.2/317.3 MB 5.3 MB/s eta 0:00:54\n",
      "     ---- ---------------------------------- 33.3/317.3 MB 5.3 MB/s eta 0:00:54\n",
      "     ---- ---------------------------------- 34.6/317.3 MB 5.3 MB/s eta 0:00:54\n",
      "     ---- ---------------------------------- 35.7/317.3 MB 5.3 MB/s eta 0:00:53\n",
      "     ---- ---------------------------------- 37.2/317.3 MB 5.4 MB/s eta 0:00:53\n",
      "     ---- ---------------------------------- 38.3/317.3 MB 5.4 MB/s eta 0:00:52\n",
      "     ---- ---------------------------------- 39.8/317.3 MB 5.4 MB/s eta 0:00:52\n",
      "     ----- --------------------------------- 41.2/317.3 MB 5.4 MB/s eta 0:00:51\n",
      "     ----- --------------------------------- 42.5/317.3 MB 5.5 MB/s eta 0:00:51\n",
      "     ----- --------------------------------- 43.8/317.3 MB 5.5 MB/s eta 0:00:50\n",
      "     ----- --------------------------------- 44.8/317.3 MB 5.5 MB/s eta 0:00:50\n",
      "     ----- --------------------------------- 46.1/317.3 MB 5.5 MB/s eta 0:00:50\n",
      "     ----- --------------------------------- 47.4/317.3 MB 5.5 MB/s eta 0:00:49\n",
      "     ----- --------------------------------- 48.8/317.3 MB 5.5 MB/s eta 0:00:49\n",
      "     ------ -------------------------------- 50.1/317.3 MB 5.5 MB/s eta 0:00:49\n",
      "     ------ -------------------------------- 51.4/317.3 MB 5.5 MB/s eta 0:00:48\n",
      "     ------ -------------------------------- 52.4/317.3 MB 5.5 MB/s eta 0:00:48\n",
      "     ------ -------------------------------- 53.7/317.3 MB 5.5 MB/s eta 0:00:48\n",
      "     ------ -------------------------------- 54.3/317.3 MB 5.5 MB/s eta 0:00:48\n",
      "     ------ -------------------------------- 55.3/317.3 MB 5.5 MB/s eta 0:00:48\n",
      "     ------ -------------------------------- 56.6/317.3 MB 5.5 MB/s eta 0:00:48\n",
      "     ------- ------------------------------- 57.9/317.3 MB 5.5 MB/s eta 0:00:48\n",
      "     ------- ------------------------------- 59.2/317.3 MB 5.5 MB/s eta 0:00:47\n",
      "     ------- ------------------------------- 60.6/317.3 MB 5.5 MB/s eta 0:00:47\n",
      "     ------- ------------------------------- 61.6/317.3 MB 5.5 MB/s eta 0:00:47\n",
      "     ------- ------------------------------- 63.2/317.3 MB 5.6 MB/s eta 0:00:46\n",
      "     ------- ------------------------------- 64.5/317.3 MB 5.6 MB/s eta 0:00:46\n",
      "     -------- ------------------------------ 65.5/317.3 MB 5.6 MB/s eta 0:00:46\n",
      "     -------- ------------------------------ 66.8/317.3 MB 5.6 MB/s eta 0:00:45\n",
      "     -------- ------------------------------ 68.2/317.3 MB 5.6 MB/s eta 0:00:45\n",
      "     -------- ------------------------------ 69.2/317.3 MB 5.6 MB/s eta 0:00:45\n",
      "     -------- ------------------------------ 70.5/317.3 MB 5.6 MB/s eta 0:00:45\n",
      "     -------- ------------------------------ 71.8/317.3 MB 5.6 MB/s eta 0:00:45\n",
      "     -------- ------------------------------ 73.1/317.3 MB 5.6 MB/s eta 0:00:44\n",
      "     --------- ----------------------------- 74.4/317.3 MB 5.6 MB/s eta 0:00:44\n",
      "     --------- ----------------------------- 75.5/317.3 MB 5.6 MB/s eta 0:00:44\n",
      "     --------- ----------------------------- 76.8/317.3 MB 5.6 MB/s eta 0:00:43\n",
      "     --------- ----------------------------- 78.1/317.3 MB 5.6 MB/s eta 0:00:43\n",
      "     --------- ----------------------------- 79.4/317.3 MB 5.6 MB/s eta 0:00:43\n",
      "     --------- ----------------------------- 80.5/317.3 MB 5.6 MB/s eta 0:00:43\n",
      "     ---------- ---------------------------- 81.8/317.3 MB 5.6 MB/s eta 0:00:42\n",
      "     ---------- ---------------------------- 83.1/317.3 MB 5.6 MB/s eta 0:00:42\n",
      "     ---------- ---------------------------- 84.4/317.3 MB 5.6 MB/s eta 0:00:42\n",
      "     ---------- ---------------------------- 85.7/317.3 MB 5.6 MB/s eta 0:00:42\n",
      "     ---------- ---------------------------- 86.8/317.3 MB 5.6 MB/s eta 0:00:41\n",
      "     ---------- ---------------------------- 88.1/317.3 MB 5.6 MB/s eta 0:00:41\n",
      "     ---------- ---------------------------- 89.1/317.3 MB 5.6 MB/s eta 0:00:41\n",
      "     ----------- --------------------------- 90.4/317.3 MB 5.7 MB/s eta 0:00:41\n",
      "     ----------- --------------------------- 91.8/317.3 MB 5.7 MB/s eta 0:00:40\n",
      "     ----------- --------------------------- 92.5/317.3 MB 5.6 MB/s eta 0:00:40\n",
      "     ----------- --------------------------- 94.1/317.3 MB 5.7 MB/s eta 0:00:40\n",
      "     ----------- --------------------------- 95.2/317.3 MB 5.6 MB/s eta 0:00:40\n",
      "     ----------- --------------------------- 96.5/317.3 MB 5.7 MB/s eta 0:00:40\n",
      "     ------------ -------------------------- 97.8/317.3 MB 5.7 MB/s eta 0:00:39\n",
      "     ------------ -------------------------- 98.8/317.3 MB 5.6 MB/s eta 0:00:39\n",
      "     ----------- -------------------------- 100.1/317.3 MB 5.6 MB/s eta 0:00:39\n",
      "     ------------ ------------------------- 100.9/317.3 MB 5.6 MB/s eta 0:00:39\n",
      "     ------------ ------------------------- 102.2/317.3 MB 5.6 MB/s eta 0:00:39\n",
      "     ------------ ------------------------- 103.5/317.3 MB 5.6 MB/s eta 0:00:38\n",
      "     ------------ ------------------------- 104.9/317.3 MB 5.7 MB/s eta 0:00:38\n",
      "     ------------ ------------------------- 106.2/317.3 MB 5.7 MB/s eta 0:00:38\n",
      "     ------------ ------------------------- 107.5/317.3 MB 5.7 MB/s eta 0:00:38\n",
      "     ------------- ------------------------ 108.8/317.3 MB 5.7 MB/s eta 0:00:37\n",
      "     ------------- ------------------------ 109.8/317.3 MB 5.7 MB/s eta 0:00:37\n",
      "     ------------- ------------------------ 111.1/317.3 MB 5.7 MB/s eta 0:00:37\n",
      "     ------------- ------------------------ 112.2/317.3 MB 5.7 MB/s eta 0:00:37\n",
      "     ------------- ------------------------ 113.5/317.3 MB 5.7 MB/s eta 0:00:36\n",
      "     ------------- ------------------------ 114.6/317.3 MB 5.7 MB/s eta 0:00:36\n",
      "     ------------- ------------------------ 115.9/317.3 MB 5.7 MB/s eta 0:00:36\n",
      "     -------------- ----------------------- 117.2/317.3 MB 5.7 MB/s eta 0:00:36\n",
      "     -------------- ----------------------- 118.5/317.3 MB 5.7 MB/s eta 0:00:36\n",
      "     -------------- ----------------------- 119.8/317.3 MB 5.7 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 120.8/317.3 MB 5.7 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 122.2/317.3 MB 5.7 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 122.9/317.3 MB 5.7 MB/s eta 0:00:35\n",
      "     -------------- ----------------------- 124.3/317.3 MB 5.7 MB/s eta 0:00:35\n",
      "     --------------- ---------------------- 125.6/317.3 MB 5.7 MB/s eta 0:00:34\n",
      "     --------------- ---------------------- 126.9/317.3 MB 5.7 MB/s eta 0:00:34\n",
      "     --------------- ---------------------- 128.2/317.3 MB 5.7 MB/s eta 0:00:34\n",
      "     --------------- ---------------------- 129.2/317.3 MB 5.7 MB/s eta 0:00:34\n",
      "     --------------- ---------------------- 130.5/317.3 MB 5.7 MB/s eta 0:00:33\n",
      "     --------------- ---------------------- 131.9/317.3 MB 5.7 MB/s eta 0:00:33\n",
      "     --------------- ---------------------- 133.2/317.3 MB 5.7 MB/s eta 0:00:33\n",
      "     ---------------- --------------------- 134.2/317.3 MB 5.7 MB/s eta 0:00:33\n",
      "     ---------------- --------------------- 135.5/317.3 MB 5.7 MB/s eta 0:00:32\n",
      "     ---------------- --------------------- 136.6/317.3 MB 5.7 MB/s eta 0:00:32\n",
      "     ---------------- --------------------- 137.9/317.3 MB 5.7 MB/s eta 0:00:32\n",
      "     ---------------- --------------------- 138.9/317.3 MB 5.7 MB/s eta 0:00:32\n",
      "     ---------------- --------------------- 140.2/317.3 MB 5.7 MB/s eta 0:00:32\n",
      "     ---------------- --------------------- 141.6/317.3 MB 5.7 MB/s eta 0:00:31\n",
      "     ----------------- -------------------- 142.6/317.3 MB 5.7 MB/s eta 0:00:31\n",
      "     ----------------- -------------------- 143.9/317.3 MB 5.7 MB/s eta 0:00:31\n",
      "     ----------------- -------------------- 145.2/317.3 MB 5.7 MB/s eta 0:00:31\n",
      "     ----------------- -------------------- 146.3/317.3 MB 5.7 MB/s eta 0:00:31\n",
      "     ----------------- -------------------- 147.6/317.3 MB 5.7 MB/s eta 0:00:30\n",
      "     ----------------- -------------------- 148.9/317.3 MB 5.7 MB/s eta 0:00:30\n",
      "     ----------------- -------------------- 150.2/317.3 MB 5.7 MB/s eta 0:00:30\n",
      "     ------------------ ------------------- 151.5/317.3 MB 5.7 MB/s eta 0:00:30\n",
      "     ------------------ ------------------- 152.8/317.3 MB 5.7 MB/s eta 0:00:29\n",
      "     ------------------ ------------------- 154.1/317.3 MB 5.7 MB/s eta 0:00:29\n",
      "     ------------------ ------------------- 155.5/317.3 MB 5.7 MB/s eta 0:00:29\n",
      "     ------------------ ------------------- 156.5/317.3 MB 5.7 MB/s eta 0:00:29\n",
      "     ------------------ ------------------- 157.8/317.3 MB 5.7 MB/s eta 0:00:28\n",
      "     ------------------- ------------------ 159.1/317.3 MB 5.7 MB/s eta 0:00:28\n",
      "     ------------------- ------------------ 160.4/317.3 MB 5.7 MB/s eta 0:00:28\n",
      "     ------------------- ------------------ 161.7/317.3 MB 5.7 MB/s eta 0:00:28\n",
      "     ------------------- ------------------ 163.1/317.3 MB 5.7 MB/s eta 0:00:27\n",
      "     ------------------- ------------------ 164.1/317.3 MB 5.7 MB/s eta 0:00:27\n",
      "     ------------------- ------------------ 165.4/317.3 MB 5.7 MB/s eta 0:00:27\n",
      "     ------------------- ------------------ 166.7/317.3 MB 5.7 MB/s eta 0:00:27\n",
      "     -------------------- ----------------- 167.8/317.3 MB 5.7 MB/s eta 0:00:27\n",
      "     -------------------- ----------------- 169.1/317.3 MB 5.7 MB/s eta 0:00:26\n",
      "     -------------------- ----------------- 170.4/317.3 MB 5.8 MB/s eta 0:00:26\n",
      "     -------------------- ----------------- 171.7/317.3 MB 5.8 MB/s eta 0:00:26\n",
      "     -------------------- ----------------- 173.0/317.3 MB 5.8 MB/s eta 0:00:26\n",
      "     -------------------- ----------------- 174.3/317.3 MB 5.8 MB/s eta 0:00:25\n",
      "     --------------------- ---------------- 175.6/317.3 MB 5.8 MB/s eta 0:00:25\n",
      "     --------------------- ---------------- 176.9/317.3 MB 5.8 MB/s eta 0:00:25\n",
      "     --------------------- ---------------- 178.3/317.3 MB 5.8 MB/s eta 0:00:24\n",
      "     --------------------- ---------------- 179.6/317.3 MB 5.8 MB/s eta 0:00:24\n",
      "     --------------------- ---------------- 180.9/317.3 MB 5.8 MB/s eta 0:00:24\n",
      "     --------------------- ---------------- 182.2/317.3 MB 5.8 MB/s eta 0:00:24\n",
      "     --------------------- ---------------- 183.2/317.3 MB 5.8 MB/s eta 0:00:23\n",
      "     ---------------------- --------------- 184.5/317.3 MB 5.8 MB/s eta 0:00:23\n",
      "     ---------------------- --------------- 185.9/317.3 MB 5.9 MB/s eta 0:00:23\n",
      "     ---------------------- --------------- 186.9/317.3 MB 5.9 MB/s eta 0:00:23\n",
      "     ---------------------- --------------- 188.2/317.3 MB 5.9 MB/s eta 0:00:22\n",
      "     ---------------------- --------------- 189.5/317.3 MB 5.9 MB/s eta 0:00:22\n",
      "     ---------------------- --------------- 190.8/317.3 MB 5.9 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 192.2/317.3 MB 5.9 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 193.5/317.3 MB 5.9 MB/s eta 0:00:22\n",
      "     ----------------------- -------------- 194.8/317.3 MB 5.9 MB/s eta 0:00:21\n",
      "     ----------------------- -------------- 196.1/317.3 MB 5.9 MB/s eta 0:00:21\n",
      "     ----------------------- -------------- 197.4/317.3 MB 5.9 MB/s eta 0:00:21\n",
      "     ----------------------- -------------- 198.7/317.3 MB 5.9 MB/s eta 0:00:21\n",
      "     ----------------------- -------------- 200.0/317.3 MB 5.9 MB/s eta 0:00:20\n",
      "     ------------------------ ------------- 201.3/317.3 MB 5.9 MB/s eta 0:00:20\n",
      "     ------------------------ ------------- 202.6/317.3 MB 5.9 MB/s eta 0:00:20\n",
      "     ------------------------ ------------- 203.9/317.3 MB 5.9 MB/s eta 0:00:20\n",
      "     ------------------------ ------------- 205.3/317.3 MB 5.9 MB/s eta 0:00:19\n",
      "     ------------------------ ------------- 206.6/317.3 MB 5.9 MB/s eta 0:00:19\n",
      "     ------------------------ ------------- 207.6/317.3 MB 5.9 MB/s eta 0:00:19\n",
      "     ------------------------- ------------ 208.9/317.3 MB 5.9 MB/s eta 0:00:19\n",
      "     ------------------------- ------------ 210.0/317.3 MB 5.9 MB/s eta 0:00:19\n",
      "     ------------------------- ------------ 211.6/317.3 MB 5.9 MB/s eta 0:00:18\n",
      "     ------------------------- ------------ 212.9/317.3 MB 5.9 MB/s eta 0:00:18\n",
      "     ------------------------- ------------ 213.9/317.3 MB 5.9 MB/s eta 0:00:18\n",
      "     ------------------------- ------------ 215.2/317.3 MB 5.9 MB/s eta 0:00:18\n",
      "     ------------------------- ------------ 216.5/317.3 MB 5.9 MB/s eta 0:00:18\n",
      "     -------------------------- ----------- 217.8/317.3 MB 5.9 MB/s eta 0:00:17\n",
      "     -------------------------- ----------- 219.2/317.3 MB 5.9 MB/s eta 0:00:17\n",
      "     -------------------------- ----------- 219.9/317.3 MB 5.9 MB/s eta 0:00:17\n",
      "     -------------------------- ----------- 220.7/317.3 MB 5.9 MB/s eta 0:00:17\n",
      "     -------------------------- ----------- 222.3/317.3 MB 5.9 MB/s eta 0:00:17\n",
      "     -------------------------- ----------- 223.3/317.3 MB 5.9 MB/s eta 0:00:16\n",
      "     -------------------------- ----------- 224.7/317.3 MB 5.9 MB/s eta 0:00:16\n",
      "     --------------------------- ---------- 226.0/317.3 MB 5.9 MB/s eta 0:00:16\n",
      "     --------------------------- ---------- 227.3/317.3 MB 5.9 MB/s eta 0:00:16\n",
      "     --------------------------- ---------- 228.3/317.3 MB 5.9 MB/s eta 0:00:16\n",
      "     --------------------------- ---------- 229.9/317.3 MB 5.9 MB/s eta 0:00:15\n",
      "     --------------------------- ---------- 230.7/317.3 MB 5.9 MB/s eta 0:00:15\n",
      "     --------------------------- ---------- 232.0/317.3 MB 5.9 MB/s eta 0:00:15\n",
      "     --------------------------- ---------- 233.3/317.3 MB 5.9 MB/s eta 0:00:15\n",
      "     ---------------------------- --------- 234.6/317.3 MB 5.9 MB/s eta 0:00:14\n",
      "     ---------------------------- --------- 235.9/317.3 MB 5.9 MB/s eta 0:00:14\n",
      "     ---------------------------- --------- 237.0/317.3 MB 5.9 MB/s eta 0:00:14\n",
      "     ---------------------------- --------- 238.0/317.3 MB 5.9 MB/s eta 0:00:14\n",
      "     ---------------------------- --------- 239.3/317.3 MB 5.9 MB/s eta 0:00:14\n",
      "     ---------------------------- --------- 240.9/317.3 MB 5.9 MB/s eta 0:00:13\n",
      "     ----------------------------- -------- 242.2/317.3 MB 5.9 MB/s eta 0:00:13\n",
      "     ----------------------------- -------- 243.5/317.3 MB 5.9 MB/s eta 0:00:13\n",
      "     ----------------------------- -------- 244.8/317.3 MB 5.9 MB/s eta 0:00:13\n",
      "     ----------------------------- -------- 245.9/317.3 MB 5.9 MB/s eta 0:00:13\n",
      "     ----------------------------- -------- 246.7/317.3 MB 5.9 MB/s eta 0:00:12\n",
      "     ----------------------------- -------- 247.7/317.3 MB 5.9 MB/s eta 0:00:12\n",
      "     ----------------------------- -------- 249.0/317.3 MB 5.9 MB/s eta 0:00:12\n",
      "     ----------------------------- -------- 250.3/317.3 MB 5.9 MB/s eta 0:00:12\n",
      "     ------------------------------ ------- 251.4/317.3 MB 5.9 MB/s eta 0:00:12\n",
      "     ------------------------------ ------- 253.0/317.3 MB 5.9 MB/s eta 0:00:11\n",
      "     ------------------------------ ------- 254.0/317.3 MB 5.9 MB/s eta 0:00:11\n",
      "     ------------------------------ ------- 255.3/317.3 MB 5.9 MB/s eta 0:00:11\n",
      "     ------------------------------ ------- 256.6/317.3 MB 5.9 MB/s eta 0:00:11\n",
      "     ------------------------------ ------- 257.9/317.3 MB 5.9 MB/s eta 0:00:11\n",
      "     ------------------------------- ------ 259.3/317.3 MB 5.9 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 260.3/317.3 MB 5.9 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 261.6/317.3 MB 5.9 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 262.7/317.3 MB 5.9 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 263.2/317.3 MB 5.9 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 263.7/317.3 MB 5.9 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 264.5/317.3 MB 5.8 MB/s eta 0:00:10\n",
      "     ------------------------------- ------ 265.6/317.3 MB 5.8 MB/s eta 0:00:09\n",
      "     ------------------------------- ------ 266.9/317.3 MB 5.8 MB/s eta 0:00:09\n",
      "     -------------------------------- ----- 268.2/317.3 MB 5.8 MB/s eta 0:00:09\n",
      "     -------------------------------- ----- 269.5/317.3 MB 5.8 MB/s eta 0:00:09\n",
      "     -------------------------------- ----- 270.8/317.3 MB 5.8 MB/s eta 0:00:08\n",
      "     -------------------------------- ----- 272.1/317.3 MB 5.8 MB/s eta 0:00:08\n",
      "     -------------------------------- ----- 273.4/317.3 MB 5.8 MB/s eta 0:00:08\n",
      "     -------------------------------- ----- 274.5/317.3 MB 5.9 MB/s eta 0:00:08\n",
      "     --------------------------------- ---- 275.8/317.3 MB 5.9 MB/s eta 0:00:08\n",
      "     --------------------------------- ---- 277.1/317.3 MB 5.9 MB/s eta 0:00:07\n",
      "     --------------------------------- ---- 278.4/317.3 MB 5.9 MB/s eta 0:00:07\n",
      "     --------------------------------- ---- 279.4/317.3 MB 5.9 MB/s eta 0:00:07\n",
      "     --------------------------------- ---- 281.0/317.3 MB 5.9 MB/s eta 0:00:07\n",
      "     --------------------------------- ---- 282.3/317.3 MB 5.9 MB/s eta 0:00:06\n",
      "     --------------------------------- ---- 283.4/317.3 MB 5.9 MB/s eta 0:00:06\n",
      "     ---------------------------------- --- 284.7/317.3 MB 5.9 MB/s eta 0:00:06\n",
      "     ---------------------------------- --- 286.0/317.3 MB 5.9 MB/s eta 0:00:06\n",
      "     ---------------------------------- --- 287.3/317.3 MB 5.9 MB/s eta 0:00:06\n",
      "     ---------------------------------- --- 288.6/317.3 MB 5.9 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 289.9/317.3 MB 5.9 MB/s eta 0:00:05\n",
      "     ---------------------------------- --- 291.2/317.3 MB 5.9 MB/s eta 0:00:05\n",
      "     ----------------------------------- -- 292.6/317.3 MB 5.9 MB/s eta 0:00:05\n",
      "     ----------------------------------- -- 293.6/317.3 MB 5.9 MB/s eta 0:00:05\n",
      "     ----------------------------------- -- 294.9/317.3 MB 5.9 MB/s eta 0:00:04\n",
      "     ----------------------------------- -- 296.2/317.3 MB 5.9 MB/s eta 0:00:04\n",
      "     ----------------------------------- -- 297.5/317.3 MB 5.9 MB/s eta 0:00:04\n",
      "     ----------------------------------- -- 298.8/317.3 MB 5.9 MB/s eta 0:00:04\n",
      "     ----------------------------------- -- 299.9/317.3 MB 5.9 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 300.9/317.3 MB 5.9 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 302.3/317.3 MB 5.9 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 303.3/317.3 MB 5.9 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 304.3/317.3 MB 5.9 MB/s eta 0:00:03\n",
      "     ------------------------------------ - 305.7/317.3 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 307.0/317.3 MB 5.9 MB/s eta 0:00:02\n",
      "     ------------------------------------ - 308.3/317.3 MB 5.9 MB/s eta 0:00:02\n",
      "     -------------------------------------  309.6/317.3 MB 5.9 MB/s eta 0:00:02\n",
      "     -------------------------------------  310.9/317.3 MB 5.9 MB/s eta 0:00:02\n",
      "     -------------------------------------  312.2/317.3 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  313.5/317.3 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  314.8/317.3 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  315.9/317.3 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  317.2/317.3 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  317.2/317.3 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  317.2/317.3 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  317.2/317.3 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  317.2/317.3 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  317.2/317.3 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  317.2/317.3 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  317.2/317.3 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------  317.2/317.3 MB 5.9 MB/s eta 0:00:01\n",
      "     -------------------------------------- 317.3/317.3 MB 5.5 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\hi\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Building wheels for collected packages: pyspark\n",
      "  Building wheel for pyspark (setup.py): started\n",
      "  Building wheel for pyspark (setup.py): finished with status 'done'\n",
      "  Created wheel for pyspark: filename=pyspark-3.5.4-py2.py3-none-any.whl size=317849808 sha256=f23ea70aaebf62eb6afdff5f6d88308a50850d41cf8a7c195b65abe543b18c71\n",
      "  Stored in directory: c:\\users\\hi\\appdata\\local\\pip\\cache\\wheels\\13\\92\\64\\da92a3521323cc629fdf25dd56eb26938e08014c1b57ad3759\n",
      "Successfully built pyspark\n",
      "Installing collected packages: pyspark\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.5.3\n",
      "    Uninstalling pyspark-3.5.3:\n",
      "      Successfully uninstalled pyspark-3.5.3\n",
      "Successfully installed pyspark-3.5.4\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d73bf9-200e-44e4-8d93-2b23d9833287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
